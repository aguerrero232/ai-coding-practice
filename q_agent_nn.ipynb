{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Ariel Guerrero\n",
    "### Q-Learning Agent with ***Neural Network***\n",
    "\n",
    "\n",
    "Implementation of Q-Learning Agent with a ***neural network*** using OpenAI Gym environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Algorithm with Neural Network\n",
    "\n",
    "***Q(s<sub>t</sub>, a<sub>t</sub>) = R<sub>t+1</sub> + gamma \* max(Q(s<sub>t+1</sub>))***\n",
    "\n",
    "* s = state\n",
    "* a = action\n",
    "* R<sub>t+1</sub> = reward\n",
    "* gamma = discount factor\n",
    "* max(Q(s<sub>t+1</sub>) = max Q value for all possible actions in state s<sub>t+1</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from collections import deque\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes=200\n",
    "test_episodes=100\n",
    "max_steps=170\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering A Frozen Lake Environment with no slippery tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# frozen lake registration for non slippery lake\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78,  # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create a new environment and to display environment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def new_env(env_name):\n",
    "    \"\"\"\n",
    "        description: Create a new train and test environment.\n",
    "        @param env_name: Name of the environment\n",
    "        @return: train and test environment\n",
    "    \"\"\"\n",
    "    env = gym.make(env_name)\n",
    "    # train_env = tf_py_environment.TFPyEnvironment(env)\n",
    "    # test_env = tf_py_environment.TFPyEnvironment(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "def env_attributes(env):\n",
    "    \"\"\"\n",
    "        description:    Prints Attributes of the environment\n",
    "        @param env:      Gym environment\n",
    "    \"\"\"\n",
    "    print(\"observation space: \", env.observation_space)\n",
    "    # number of actions\n",
    "    if type(\n",
    "            env.action_space) == gym.spaces.discrete.Discrete:\n",
    "        print(\"action space: \", env.action_space)\n",
    "    else:\n",
    "        print(\"action range: \",\n",
    "              env.action_space.low, env.action_space.high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definining the Agent and Q-Agent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Agent\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "        description:    Agent with discrete or continuous action space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # is the agent discrete or continuous?\n",
    "        self.is_discrete = type(\n",
    "            env.action_space) == gym.spaces.discrete.Discrete\n",
    "\n",
    "        # if discrete, get the action size\n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "\n",
    "        env_attributes(env)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        if self.is_discrete:\n",
    "            action = random.randrange(self.action_size)\n",
    "        else:\n",
    "            action = np.random.uniform(\n",
    "                self.action_low,\n",
    "                self.action_high,\n",
    "                self.action_shape\n",
    "            )\n",
    "\n",
    "        return action\n",
    "\n",
    "\n",
    "# Q Learning Agent with Neural Network\n",
    "class QNAgent(Agent):\n",
    "    def __init__(self, env, discount_rate=0.90, learning_rate=0.05, epsilon=.99):\n",
    "        \"\"\"\n",
    "            description: Q Learning Agent with Neural Network\n",
    "            @param env: openai Gym environment\n",
    "            @param discount_rate: how much future values lose weight based on how far they are\n",
    "            @param learning_rate: rate at which an algorithm adjusts its estimates based on the new information\n",
    "            @param epsilon: probability of prioritizing an exploritory action over a policy action (explore values vs acting greedy)\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        # action size is already defined in the parent Agent class\n",
    "        # state size\n",
    "        self.state_size = env.observation_space.n\n",
    "        # learning rate (alpha)\n",
    "        self.learning_rate = learning_rate\n",
    "        # discount rate (gamma)\n",
    "        self.discount_rate = discount_rate\n",
    "        # epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.min_epsilon = 0.01\n",
    "        self.max_epsilon = 1.0\n",
    "        self.epsilon_decay = .999\n",
    "        # self.epsilon_lst = []\n",
    "        # memory\n",
    "        self.memory = deque(maxlen=2500)        \n",
    "        # building the model\n",
    "        self.build_model()\n",
    "\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.model = keras.Sequential()\n",
    "        self.model.add(Dense(10, input_dim=self.state_size, activation='relu'))\n",
    "        # self.model.add(Dense(4, activation='relu'))\n",
    "        self.model.add(Dense(self.action_size, activation='linear'))\n",
    "        self.model.compile(loss='mse', optimizer=Adam(learning_rate=self.learning_rate))\n",
    "        print(self.model.summary())\n",
    "        keras.utils.plot_model(self.model, show_layer_names=False)\n",
    "\n",
    "\n",
    "    def add_memory(self, new_state, reward, done, state, action):\n",
    "        self.memory.append((new_state, reward, done, state, action))\n",
    "\n",
    "\n",
    "    def get_action(self, state):\n",
    "         # return a random action with probability epsilon else return the greedy action\n",
    "        action_explore, action_greedy = np.random.randint(0, 4), self.predict(state)\n",
    "        return action_explore if np.random.rand() < self.epsilon else action_greedy \n",
    "\n",
    "\n",
    "    def predict(self, state):\n",
    "        return np.argmax(self.model.predict(np.array(state)))\n",
    "\n",
    "\n",
    "    def train(self, experience):\n",
    "        new_state, reward, done, state, action = experience\n",
    "        target = reward + self.discount_rate * np.argmax(self.model.predict(new_state))\n",
    "        target_f = self.model.predict(state)   \n",
    "        target_f[0][action] = target\n",
    "        \n",
    "        self.model.fit(state, target_f, batch_size=2, epochs=22, verbose=0)\n",
    "\n",
    "        if self.epsilon > self.min_epsilon and done:\n",
    "            self.model.evaluate(state, target_f, verbose=0)\n",
    "            self.epsilon = (self.max_epsilon - self.min_epsilon) *  self.epsilon * self.epsilon_decay \n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Agent and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lake is slippery (hard)\n",
    "# env_name = 'FrozenLake-v1'\n",
    "#  lake is not slippery (easy)\n",
    "env_name = 'FrozenLakeNoSlip-v0'\n",
    "\n",
    "# create new environments\n",
    "env = new_env(env_name)\n",
    "\n",
    "#  new QAgent with neural network\n",
    "agent = QNAgent(env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "![model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "reward_lst=[]\n",
    "\n",
    "for episode in range(train_episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "    state_arr=np.zeros(agent.state_size)\n",
    "    state_arr[state] = 1\n",
    "    state = np.reshape(state_arr, [1, agent.state_size])\n",
    "    reward = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        new_state_arr = np.zeros(agent.state_size)\n",
    "        new_state_arr[new_state] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, agent.state_size])\n",
    "        agent.train(experience=(new_state, reward, done, state, action))\n",
    "        state = new_state\n",
    "        \n",
    "        env.render()\n",
    "        # time.sleep(.05)\n",
    "        step += 1\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    print(f'episode: {episode}/{train_episodes}, step: {step}, reward: {reward}, epsilon: {agent.epsilon}')\n",
    "    reward_lst.append(reward)\n",
    "    # print(f\"agent memory len: {len(agent.memory)}\")\n",
    "    # if len(agent.memory) > batch_size:\n",
    "    #         agent.train_replay(batch_size)   \n",
    " \n",
    "print(f'Training score:  {round(100*np.mean(reward_lst), 1)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "successes=[]\n",
    "for episode in range(test_episodes):\n",
    "    \n",
    "    state = env.reset()\n",
    "    state_arr=np.zeros(agent.state_size)\n",
    "    state_arr[state] = 1\n",
    "    state = np.reshape(state_arr, [1, agent.state_size])\n",
    "    reward = 0\n",
    "    step = 0\n",
    "    done = False\n",
    "\n",
    "    state_lst = []\n",
    "    state_lst.append(state)\n",
    "\n",
    "    while not done and step in < max_steps:\n",
    "        action = agent.pred(state)\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "\n",
    "        new_state_arr = np.zeros(state_size)\n",
    "        new_state_arr[new_state] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, state_size])\n",
    "        \n",
    "        state = new_state\n",
    "        state_lst.append(state)\n",
    "        \n",
    "        env.render()\n",
    "        # time.sleep(.05)\n",
    "        clear_output(wait=True)\n",
    "        step += 1\n",
    "        \n",
    "    print(f'episode: {episode}/{test_episodes}, step: {step}, reward: {reward}, epsilon: {agent.epsilon}')\n",
    "    successes.append(reward)\n",
    "env.close()\n",
    "\n",
    "print(f'Testing score: {int(100*np.mean(test_wins))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "keep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # def train_replay(self, batch_size):\n",
    "    #     minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "    #     for new_state, reward, done, state, action in minibatch:\n",
    "    #         target = reward\n",
    "    #         if not done:\n",
    "    #             target = reward + self.discount_rate * np.argmax(self.model.predict(new_state))\n",
    "    #         target_f = self.model.predict(state)\n",
    "    #         target_f[0][action] = target\n",
    "    #         self.model.fit(state, target_f, epochs=10, verbose=0)\n",
    "\n",
    "    #     if self.epsilon > self.min_epsilon:\n",
    "    #         self.epsilon *= self.epsilon_decay \n",
    "\n",
    "    #     self.epsilon_lst.append(self.epsilon)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
