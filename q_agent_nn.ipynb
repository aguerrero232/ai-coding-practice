{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Ariel Guerrero\n",
    "### Q-Learning Agent with ***Neural Network***\n",
    "\n",
    "\n",
    "Implementation of Q-Learning Agent with a ***neural network*** using OpenAI Gym environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Algorithm\n",
    "\n",
    "***Q(s<sub>t</sub>, a<sub>t</sub>) = R<sub>t+1</sub> + gamma*** * ***max(Q(s<sub>t+1</sub>))***\n",
    "\n",
    "* s = state\n",
    "* a = action\n",
    "* R<sub>t+1</sub> = reward\n",
    "* gamma = discount factor\n",
    "* max(Q(s<sub>t+1</sub>) = max Q value for all possible actions in state s<sub>t+1</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 15:46:41.403712: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-05 15:46:41.403761: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes=200\n",
    "test_episodes=100\n",
    "max_steps=170\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering A Frozen Lake Environment with no slippery tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozen lake registration for non slippery lake\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78,  # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create a new environment and to display environment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_env(env_name):\n",
    "    \"\"\"\n",
    "        description: Create a new train and test environment.\n",
    "        @param env_name: Name of the environment\n",
    "        @return: train and test environment\n",
    "    \"\"\"\n",
    "    env_train = gym.make(env_name)\n",
    "    env_test = gym.make(env_name)\n",
    "    return env_train, env_test\n",
    "\n",
    "def env_attributes(env):\n",
    "    \"\"\"\n",
    "        description:    Prints Attributes of the environment\n",
    "        @param env:      Gym environment\n",
    "    \"\"\"\n",
    "    print(\"observation space: \", env.observation_space)\n",
    "    # number of actions\n",
    "    if type(\n",
    "            env.action_space) == gym.spaces.discrete.Discrete:\n",
    "        print(\"action space: \", env.action_space)\n",
    "    else:\n",
    "        print(\"action range: \",\n",
    "              env.action_space.low, env.action_space.high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definining the Agent and QNN-Agent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Agent\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "        description:    Agent with discrete or continuous action space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # is the agent discrete or continuous?\n",
    "        self.is_discrete = type(\n",
    "            env.action_space) == gym.spaces.discrete.Discrete\n",
    "        # if discrete, get the action size\n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "        self.env = env\n",
    "        env_attributes(env)\n",
    "\n",
    "    def get_action(self):\n",
    "        if self.is_discrete:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.random.uniform(\n",
    "                self.action_low,\n",
    "                self.action_high,\n",
    "                self.action_shape\n",
    "            )\n",
    "        return action\n",
    "\n",
    "# Q Learning Agent with Neural Network\n",
    "class QNNAgent(Agent):\n",
    "    def __init__(self, env, discount_rate=0.90, learning_rate=0.01, epsilon=.99):\n",
    "        \"\"\"\n",
    "            description: Q Learning Agent with Neural Network\n",
    "            @param env: openai Gym environment\n",
    "            @param discount_rate: how much future values lose weight based on how far they are\n",
    "            @param learning_rate: rate at which an algorithm adjusts its estimates based on the new information\n",
    "            @param epsilon: probability of prioritizing an exploritory action over a policy action (explore values vs acting greedy)\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        # action size is already defined in the parent Agent class\n",
    "        # state size\n",
    "        self.state_size = env.observation_space.n\n",
    "        # learning rate (alpha)\n",
    "        self.learning_rate = learning_rate\n",
    "        # discount rate (gamma)\n",
    "        self.discount_rate = discount_rate\n",
    "        # epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = .985\n",
    "        # memory\n",
    "        self.memory = deque(maxlen=2500)        \n",
    "        # building the model\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Dense(16, input_dim=self.state_size, activation='relu'),\n",
    "            keras.layers.Dense(self.action_size, activation='sigmoid')\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=self.learning_rate), metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "        keras.utils.plot_model(self.model, show_layer_names=False)\n",
    "\n",
    "    def add_memory(self, new_state, reward, done, state, action):\n",
    "        self.memory.append((new_state, reward, done, state, action))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return super().get_action() if np.random.rand() < self.epsilon else self.predict(state)\n",
    "\n",
    "    def predict(self, state):\n",
    "        return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def train(self, experience):\n",
    "        new_state, reward, done, state, action = experience\n",
    "        # target = reward + (gamma * max(Q(s')))\n",
    "        target = reward + self.discount_rate * self.predict(new_state)\n",
    "        q_next = self.model.predict(state)     \n",
    "        q_next[0][action] = target\n",
    "        self.model.fit(state, q_next, batch_size=2, epochs=120, verbose=0)\n",
    "        if done:\n",
    "            self.epsilon *= self.epsilon_decay "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Agent and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Discrete(16)\n",
      "action space:  Discrete(4)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 16)                272       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 340\n",
      "Trainable params: 340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-05 15:46:44.805178: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2022-04-05 15:46:44.809086: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-05 15:46:44.809134: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-05 15:46:44.809171: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-05 15:46:44.811195: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-05 15:46:44.811234: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-05 15:46:44.811281: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-05 15:46:44.811287: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-05 15:46:44.812115: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#  lake is slippery (hard)\n",
    "# env_name = 'FrozenLake-v1'\n",
    "#  lake is not slippery (easy)\n",
    "env_name = 'FrozenLakeNoSlip-v0'\n",
    "# create new environments\n",
    "env_train, env_test = new_env(env_name)\n",
    "#  new QAgent with neural network\n",
    "agent = QNNAgent(env_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "![model](model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 23/200, steps taken: 6, total reward: 1.0, epsilon: 0.6993079991899932\n"
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for episode in range(train_episodes):\n",
    "\n",
    "    state = env_train.reset()   \n",
    "    state_arr = np.zeros(agent.state_size)\n",
    "    state = np.reshape(state_arr, [1, agent.state_size])\n",
    "    reward = step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done, info = env_train.step(action)\n",
    "        new_state_arr = np.zeros(agent.state_size)\n",
    "        new_state_arr[new_state] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, agent.state_size])\n",
    "        agent.train(experience=(new_state, reward, done, state, action))\n",
    "        state = new_state\n",
    "        # not rendering since it takes forever \n",
    "        env_train.render()\n",
    "        # time.sleep(.01)\n",
    "        step += 1\n",
    "        \n",
    "    clear_output(wait=True)\n",
    "    total_reward += reward\n",
    "\n",
    "    print(f'episode: {episode+1}/{train_episodes}, steps taken: {step}, total reward: {total_reward}, epsilon: {agent.epsilon}')\n",
    "\n",
    "env_train.close()\n",
    "\n",
    "print(f'Training score:  {100 * (total_reward / train_episodes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_complete = 0\n",
    "for episode in range(test_episodes):\n",
    "    \n",
    "    state = env_test.reset()\n",
    "    state_arr=np.zeros(agent.state_size)\n",
    "    state = np.reshape(state_arr, [1, agent.state_size])\n",
    "    reward = step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        \n",
    "        action = agent.predict(state)\n",
    "\n",
    "        new_state, reward, done, info = env_test.step(action)\n",
    "        new_state_arr = np.zeros(agent.state_size)\n",
    "        new_state_arr[new_state] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, agent.state_size])\n",
    "        state = new_state\n",
    "        \n",
    "        env_test.render()\n",
    "        time.sleep(.05)\n",
    "        step += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    total_complete += reward\n",
    "    print(f'episode: {episode}/{test_episodes}, steps taken: {step}, completions: {total_complete}, epsilon: {agent.epsilon}')\n",
    "\n",
    "env_test.close()\n",
    "\n",
    "print(f'Testing score: {(total_complete/test_episodes) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitted code snippets, useful to keep just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        # print(\"action: \", action, \" reward: \", reward, \" done: \", done, \" info: \", info, \" new_state: \", new_state, \" new_state_arr: \", new_state_arr, \" state: \", state, \" state_arr: \", state_arr)\n",
    "        \n",
    "        # print(f\"agent memory len: {len(agent.memory)}\")\n",
    "        # if len(agent.memory) > batch_size:\n",
    "        #         agent.train_replay(batch_size)   \n",
    "\n",
    "        # def train_replay(self, batch_size):\n",
    "        #     minibatch = random.sample(self.memory, batch_size)\n",
    "\n",
    "        #     for new_state, reward, done, state, action in minibatch:\n",
    "        #         target = reward\n",
    "        #         if not done:\n",
    "        #             target = reward + self.discount_rate * np.argmax(self.model.predict(new_state))\n",
    "        #         target_f = self.model.predict(state)\n",
    "        #         target_f[0][action] = target\n",
    "        #         self.model.fit(state, target_f, epochs=10, verbose=0)\n",
    "\n",
    "        #     if self.epsilon > self.min_epsilon:\n",
    "        #         self.epsilon *= self.epsilon_decay \n",
    "\n",
    "        #     self.epsilon_lst.append(self.epsilon)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
