{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author: Ariel Guerrero\n",
    "### Q-Learning Agent with ***Neural Network***\n",
    "\n",
    "\n",
    "Implementation of Q-Learning Agent with a ***neural network*** using OpenAI Gym environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q Learning Algorithm\n",
    "\n",
    "***Q(s<sub>t</sub>, a<sub>t</sub>) = R<sub>t+1</sub> + gamma*** * ***max(Q(s<sub>t+1</sub>))***\n",
    "\n",
    "* s = state\n",
    "* a = action\n",
    "* R<sub>t+1</sub> = reward\n",
    "* gamma = discount factor\n",
    "* max(Q(s<sub>t+1</sub>) = max Q value for all possible actions in state s<sub>t+1</sub>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "from gym.envs.registration import register\n",
    "from IPython.display import clear_output\n",
    "from collections import deque\n",
    "\n",
    "import pygame\n",
    "\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sometimes pygame doesn't close properly, ***run this cell to force close***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pygame is running, quit it\n",
    "if pygame.get_init():\n",
    "    pygame.quit()\n",
    "    pygame.register_quit(sys.exit)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_episodes=600\n",
    "test_episodes=200\n",
    "max_steps=200\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering A Frozen Lake Environment with no slippery tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frozen lake registration for non slippery lake\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNoSlip-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name': '4x4', 'is_slippery': False},\n",
    "        max_episode_steps=100,\n",
    "        reward_threshold=0.78,  # optimum = .8196\n",
    "    )\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions to create a new environment and to display environment information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_env(env_name):\n",
    "    \"\"\"\n",
    "        description: Create a new train and test environment.\n",
    "        @param env_name: Name of the environment\n",
    "        @return: train and test environment\n",
    "    \"\"\"\n",
    "    env_train = gym.make(env_name)\n",
    "    env_test = gym.make(env_name)\n",
    "    return env_train, env_test\n",
    "\n",
    "def env_attributes(env):\n",
    "    \"\"\"\n",
    "        description:    Prints Attributes of the environment\n",
    "        @param env:      Gym environment\n",
    "    \"\"\"\n",
    "    print(\"observation space: \", env.observation_space)\n",
    "    # number of actions\n",
    "    if type(\n",
    "            env.action_space) == gym.spaces.discrete.Discrete:\n",
    "        print(\"action space: \", env.action_space)\n",
    "    else:\n",
    "        print(\"action range: \",\n",
    "              env.action_space.low, env.action_space.high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definining the Agent and QNN-Agent classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Agent\n",
    "class Agent():\n",
    "    \"\"\"\n",
    "        description:    Agent with discrete or continuous action space\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        # is the agent discrete or continuous?\n",
    "        self.is_discrete = type(\n",
    "            env.action_space) == gym.spaces.discrete.Discrete\n",
    "        # if discrete, get the action size\n",
    "        if self.is_discrete:\n",
    "            self.action_size = env.action_space.n\n",
    "        else:\n",
    "            self.action_low = env.action_space.low\n",
    "            self.action_high = env.action_space.high\n",
    "            self.action_shape = env.action_space.shape\n",
    "        self.env = env\n",
    "        env_attributes(env)\n",
    "\n",
    "    def get_action(self):\n",
    "        if self.is_discrete:\n",
    "            action = self.env.action_space.sample()\n",
    "        else:\n",
    "            action = np.random.uniform(\n",
    "                self.action_low,\n",
    "                self.action_high,\n",
    "                self.action_shape\n",
    "            )\n",
    "        return action\n",
    "\n",
    "# Q Learning Agent with Neural Network\n",
    "class QNNAgent(Agent):\n",
    "    def __init__(self, env, discount_rate=0.90, learning_rate=0.01, epsilon=.99):\n",
    "        \"\"\"\n",
    "            description: Q Learning Agent with Neural Network\n",
    "            @param env: openai Gym environment\n",
    "            @param discount_rate: how much future values lose weight based on how far they are\n",
    "            @param learning_rate: rate at which an algorithm adjusts its estimates based on the new information\n",
    "            @param epsilon: probability of prioritizing an exploritory action over a policy action (explore values vs acting greedy)\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        # action size is already defined in the parent Agent class\n",
    "        # state size\n",
    "        self.state_size = env.observation_space.n\n",
    "        # learning rate (alpha)\n",
    "        self.learning_rate = learning_rate\n",
    "        # discount rate (gamma)\n",
    "        self.discount_rate = discount_rate\n",
    "        # epsilon\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = .99\n",
    "        # memory\n",
    "        self.memory = deque(maxlen=2500)        \n",
    "        # building the model\n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.model = keras.Sequential([\n",
    "            keras.layers.Dense(16, input_dim=self.state_size, activation='relu'),\n",
    "            keras.layers.Dense(self.action_size, activation='sigmoid')\n",
    "        ])\n",
    "        self.model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=self.learning_rate), metrics=['accuracy'])\n",
    "        print(self.model.summary())\n",
    "        keras.utils.plot_model(self.model, show_layer_names=False)\n",
    "\n",
    "    def print_model_bias(self):\n",
    "        for layer in self.model.layers:\n",
    "            print(layer.name)\n",
    "            print(f\"shape: {layer.get_weights()[1].shape}\")\n",
    "            print(f\"bias: {layer.get_weights()[1]}\")\n",
    "\n",
    "    def print_model_weights(self):\n",
    "        for layer in self.model.layers:\n",
    "            print(layer.name)\n",
    "            print(f\"shape: {layer.get_weights()[0].shape}\")\n",
    "            print(f\"weights: {layer.get_weights()[0]}\")\n",
    "\n",
    "    def add_memory(self, new_state, reward, done, state, action):\n",
    "        self.memory.append((new_state, reward, done, state, action))\n",
    "\n",
    "    def get_action(self, state):\n",
    "        return super().get_action() if np.random.rand() < self.epsilon else self.predict(state)\n",
    "\n",
    "    def predict(self, state):\n",
    "        return np.argmax(self.model.predict(state))\n",
    "\n",
    "    def train(self, experience):\n",
    "        new_state, reward, done, state, action = experience\n",
    "        # target = reward + (gamma * max(Q(s')))\n",
    "        target = reward + self.discount_rate * self.predict(new_state)\n",
    "        q_next = self.model.predict(state) if not done else np.zeros((1, self.action_size))\n",
    "\n",
    "        if done and reward !=1:\n",
    "           q_update = (target - action) * self.learning_rate\n",
    "        elif not done and reward != 1:\n",
    "            q_update = target\n",
    "        else:\n",
    "            q_update = target * (self.learning_rate + 1)\n",
    "\n",
    "        q_next[0][action] = q_update\n",
    "        \n",
    "        self.model.fit(state, q_next, epochs=60, batch_size=5, verbose=0)\n",
    "        \n",
    "        if done:\n",
    "            self.epsilon *= self.epsilon_decay \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Agent and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lake is slippery (hard)\n",
    "# env_name = 'FrozenLake-v1'\n",
    "#  lake is not slippery (easy)\n",
    "env_name = 'FrozenLakeNoSlip-v0'\n",
    "# create new environments\n",
    "env_train, env_test = new_env(env_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment\n",
    "\n",
    "![](img/frozen_lake_env.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space:  Discrete(16)\n",
      "action space:  Discrete(4)\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 16)                272       \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4)                 68        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 340\n",
      "Trainable params: 340\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"
     ]
    }
   ],
   "source": [
    "#  new QAgent with neural network\n",
    "agent = QNNAgent(env_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Model\n",
    "![model](img/model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Driver Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 95/600, steps taken: 15, total reward: 2.0, epsilon: 0.38104711810454983\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb Cell 22'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000020vscode-remote?line=14'>15</a>\u001b[0m state_arr[new_state] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000020vscode-remote?line=15'>16</a>\u001b[0m new_state \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(state_arr, [\u001b[39m1\u001b[39m, agent\u001b[39m.\u001b[39mstate_size])\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000020vscode-remote?line=16'>17</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(experience\u001b[39m=\u001b[39;49m(new_state, reward, done, state, action))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000020vscode-remote?line=17'>18</a>\u001b[0m state \u001b[39m=\u001b[39m new_state\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000020vscode-remote?line=18'>19</a>\u001b[0m \u001b[39m# not rendering since it takes forever \u001b[39;00m\n",
      "\u001b[1;32m/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb Cell 14'\u001b[0m in \u001b[0;36mQNNAgent.train\u001b[0;34m(self, experience)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000013vscode-remote?line=89'>90</a>\u001b[0m \u001b[39m# target = reward + (gamma * max(Q(s')))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000013vscode-remote?line=90'>91</a>\u001b[0m target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscount_rate \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict(new_state)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000013vscode-remote?line=91'>92</a>\u001b[0m q_next \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state) \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mzeros((\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_size))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000013vscode-remote?line=93'>94</a>\u001b[0m \u001b[39mif\u001b[39;00m done \u001b[39mand\u001b[39;00m reward \u001b[39m!=\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu20.04lts/home/vrixl/code/ai-coding-practice/q_agent_nn.ipynb#ch0000013vscode-remote?line=94'>95</a>\u001b[0m    q_update \u001b[39m=\u001b[39m (target \u001b[39m-\u001b[39m action) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlearning_rate\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=61'>62</a>\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=62'>63</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=63'>64</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/training.py:1975\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1964'>1965</a>\u001b[0m   callbacks \u001b[39m=\u001b[39m callbacks_module\u001b[39m.\u001b[39mCallbackList(\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1965'>1966</a>\u001b[0m       callbacks,\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1966'>1967</a>\u001b[0m       add_history\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1970'>1971</a>\u001b[0m       epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1971'>1972</a>\u001b[0m       steps\u001b[39m=\u001b[39mdata_handler\u001b[39m.\u001b[39minferred_steps)\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1973'>1974</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_predict_function()\n\u001b[0;32m-> <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1974'>1975</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict_counter\u001b[39m.\u001b[39;49massign(\u001b[39m0\u001b[39;49m)\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1975'>1976</a>\u001b[0m callbacks\u001b[39m.\u001b[39mon_predict_begin()\n\u001b[1;32m   <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/keras/engine/training.py?line=1976'>1977</a>\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:953\u001b[0m, in \u001b[0;36mBaseResourceVariable.assign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=949'>950</a>\u001b[0m   assign_op \u001b[39m=\u001b[39m gen_resource_variable_ops\u001b[39m.\u001b[39massign_variable_op(\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=950'>951</a>\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, value_tensor, name\u001b[39m=\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=951'>952</a>\u001b[0m   \u001b[39mif\u001b[39;00m read_value:\n\u001b[0;32m--> <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=952'>953</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_read(assign_op)\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=953'>954</a>\u001b[0m \u001b[39mreturn\u001b[39;00m assign_op\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py:911\u001b[0m, in \u001b[0;36mBaseResourceVariable._lazy_read\u001b[0;34m(self, op)\u001b[0m\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=903'>904</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_lazy_read\u001b[39m(\u001b[39mself\u001b[39m, op):\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=904'>905</a>\u001b[0m   variable_accessed(\u001b[39mself\u001b[39m)\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=905'>906</a>\u001b[0m   \u001b[39mreturn\u001b[39;00m _UnreadVariable(\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=906'>907</a>\u001b[0m       handle\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle,\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=907'>908</a>\u001b[0m       dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype,\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=908'>909</a>\u001b[0m       shape\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape,\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=909'>910</a>\u001b[0m       in_graph_mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_graph_mode,\n\u001b[0;32m--> <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=910'>911</a>\u001b[0m       deleter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_handle_deleter \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_in_graph_mode \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=911'>912</a>\u001b[0m       parent_op\u001b[39m=\u001b[39mop,\n\u001b[1;32m    <a href='file:///home/vrixl/.local/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py?line=912'>913</a>\u001b[0m       unique_id\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unique_id)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_reward = 0\n",
    "for episode in range(train_episodes):\n",
    "\n",
    "    state = env_train.reset()   \n",
    "    state_arr = np.zeros(agent.state_size)\n",
    "    state_arr[state] = 1\n",
    "    state = np.reshape(state_arr, [1, agent.state_size])\n",
    "    reward = step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, done, info = env_train.step(action)\n",
    "        state_arr = np.zeros(agent.state_size)\n",
    "        state_arr[new_state] = 1\n",
    "        new_state = np.reshape(state_arr, [1, agent.state_size])\n",
    "        agent.train(experience=(new_state, reward, done, state, action))\n",
    "        state = new_state\n",
    "        # not rendering since it takes forever \n",
    "        env_train.render()\n",
    "        # time.sleep(.01)\n",
    "        step += 1\n",
    "    clear_output(wait=True)\n",
    "    total_reward += reward\n",
    "    print(f'episode: {episode+1}/{train_episodes}, steps taken: {step}, total reward: {total_reward}, epsilon: {agent.epsilon}')\n",
    "    # print(f'----------------------------- model bias --------------------------------')\n",
    "    # agent.print_model_bias()\n",
    "env_train.close()\n",
    "print(f'Training score:  {100 * (total_reward / train_episodes)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_complete = 0\n",
    "for episode in range(test_episodes):\n",
    "    \n",
    "    state = env_test.reset()\n",
    "    state_arr=np.zeros(agent.state_size)\n",
    "    state_arr[state] = 1\n",
    "    state = np.reshape(state_arr, [1, agent.state_size])\n",
    "    reward = step = 0\n",
    "    done = False\n",
    "\n",
    "    while not done and step < max_steps:\n",
    "        \n",
    "        action = agent.predict(state)\n",
    "\n",
    "        new_state, reward, done, info = env_test.step(action)\n",
    "        new_state_arr = np.zeros(agent.state_size)\n",
    "        new_state_arr[new_state] = 1\n",
    "        new_state = np.reshape(new_state_arr, [1, agent.state_size])\n",
    "        state = new_state\n",
    "        \n",
    "        env_test.render()\n",
    "        time.sleep(.05)\n",
    "        step += 1\n",
    "\n",
    "    clear_output(wait=True)\n",
    "    total_complete += reward\n",
    "    print(f'episode: {episode}/{test_episodes}, steps taken: {step}, completions: {total_complete}, epsilon: {agent.epsilon}')\n",
    "\n",
    "env_test.close()\n",
    "\n",
    "print(f'Testing score: {(total_complete/test_episodes) * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitted code snippets, useful to keep just in case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "        # print(\"action: \", action, \" reward: \", reward, \" done: \", done, \" info: \", info, \" new_state: \", new_state, \" new_state_arr: \", new_state_arr, \" state: \", state, \" state_arr: \", state_arr)\n",
    "        # print(f\"agent memory len: {len(agent.memory)}\")\n",
    "        # if len(agent.memory) > batch_size:\n",
    "        #         agent.train_replay(batch_size)   \n",
    "        # def train_replay(self, batch_size):\n",
    "        #     minibatch = random.sample(self.memory, batch_size)\n",
    "        #     for new_state, reward, done, state, action in minibatch:\n",
    "        #         target = reward\n",
    "        #         if not done:\n",
    "        #             target = reward + self.discount_rate * np.argmax(self.model.predict(new_state))\n",
    "        #         target_f = self.model.predict(state)\n",
    "        #         target_f[0][action] = target\n",
    "        #         self.model.fit(state, target_f, epochs=10, verbose=0)\n",
    "        #     if self.epsilon > self.min_epsilon:\n",
    "        #         self.epsilon *= self.epsilon_decay \n",
    "        #     self.epsilon_lst.append(self.epsilon)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(f\"state: {state}\")\n",
    "        # print(f\"new_state: {new_state}\")\n",
    "        # print(f\"target: {target}\")\n",
    "        # print(f\"action: {action}\")\n",
    "        # print(f\"q_next: {q_next}\")\n",
    "        # print(f\"q_next[action]: {q_next[0][action]}\")\n",
    "        # print(f\"q_update: {q_update}\")\n",
    "        # print(f\"updated q_next[action]: {q_next[0][action]}\")\n",
    "        # print(f'state {state}, q_next {q_next}')\n",
    "        # wait 5 sec\n",
    "        # time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
